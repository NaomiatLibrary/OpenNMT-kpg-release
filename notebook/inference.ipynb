{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "exec('from __future__ import unicode_literals')\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import random\n",
    "\n",
    "module_path = os.path.abspath(os.path.join('../'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "module_path = os.path.abspath(os.path.join('../onmt'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "\n",
    "from itertools import repeat\n",
    "\n",
    "from onmt.utils.logging import init_logger\n",
    "from onmt.utils.misc import split_corpus\n",
    "from onmt.translate.translator import build_translator\n",
    "\n",
    "import onmt.opts as opts\n",
    "from onmt.utils.parse import ArgumentParser\n",
    "from kp_gen_eval import _get_parser\n",
    "\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "stoplist = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load a paragraph from 10-K docs (assume current directory is OpenNMT-kpg/notebook/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/rui.meng/project/OpenNMT-kpg/data/10k\n",
      "Loaded #(10k docs)=0\n"
     ]
    }
   ],
   "source": [
    "data_root_path = '/export/share/rmeng/project/OpenNMT-kpg/data/10k'\n",
    "data_root_path = '../data/10k'\n",
    "print(os.path.abspath(data_root_path))\n",
    "doc_dicts = []\n",
    "for subdir, dirs, files in os.walk(data_root_path):\n",
    "    for file in files:\n",
    "        filepath = subdir + os.sep + file\n",
    "        if filepath.endswith(\".txt\"):\n",
    "#             print(filepath)\n",
    "            text = open(filepath, 'r').readlines()\n",
    "            pars = [l for l in text if len(l.split()) > 20]\n",
    "            doc = {'name': file, 'path': filepath, 'text': text, 'paragraph': pars}\n",
    "#             print('#line=%d, #par=%d' % (len(text), len(pars)))\n",
    "            doc_dicts.append(doc)\n",
    "    \n",
    "print('Loaded #(10k docs)=%d' % (len(doc_dicts)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Sample a paragraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7705\n",
      "3\n",
      "CSX was incorporated in 1978 under Virginia law. In 1980, the Company completed the merger of the Chessie System and Seaboard Coast Line Industries into CSX. The merger allowed the Company to connect northern population centers and Appalachian coal fields to growing southeastern markets. Later, the Company’s acquisition of key portions of Conrail, Inc. (\"Conrail\") allowed CSXT to link the northeast, including New England and the New York metropolitan area, with Chicago and midwestern markets as well as the growing areas in the Southeast already served by CSXT. This current rail network allows the Company to directly serve every major market in the eastern United States with safe, dependable, environmentally responsible and fuel efficient freight transportation and intermodal service.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "doc_id = random.randint(0, len(doc_dicts))\n",
    "doc = doc_dicts[doc_id]\n",
    "pars = doc['paragraph']\n",
    "par_id = random.randint(0, len(doc_dicts[doc_id]['paragraph']))\n",
    "text_to_extract = pars[par_id]\n",
    "print(doc_id)\n",
    "print(par_id)\n",
    "print(text_to_extract)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-2a5a2f6deefa>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mdoc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdoc_dicts\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdoc_id\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mpars\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdoc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'paragraph'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mpar\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpars\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpar_id\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpar_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "# doc_id = 9539\n",
    "# par_id = 67\n",
    "doc = doc_dicts[doc_id]\n",
    "pars = doc['paragraph']\n",
    "par = pars[par_id]\n",
    "print(doc_id)\n",
    "print(par_id)\n",
    "print(par)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Or load a PROD description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROD_path = '/export/share/rmeng/project/OpenNMT-kpg/data/SF_Prod'\n",
    "\n",
    "prod_dicts = []\n",
    "for subdir, dirs, files in os.walk(PROD_path):\n",
    "    for file in files:\n",
    "        filepath = subdir + os.sep + file\n",
    "        text = open(filepath, 'r').readlines()\n",
    "        text = '\\n'.join([l.strip() for l in text])\n",
    "        doc = {'name': file, 'path': filepath, 'text': text}\n",
    "        prod_dicts.append(doc)\n",
    "\n",
    "print('Loaded #(PROD docs)=%d' % (len(prod_dicts)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_id = random.randint(0, len(prod_dicts))\n",
    "doc = prod_dicts[doc_id]\n",
    "text_to_extract = doc['text']\n",
    "print(doc_id)\n",
    "print(doc['name'])\n",
    "print(text_to_extract)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Supervised Deep Keyphrase Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = _get_parser()\n",
    "config_path = '../config/translate/config-rnn-keyphrase.yml'\n",
    "one2one_ckpt_path = '../models/keyphrase/meng17-one2one-kp20k-topmodels/kp20k-meng17-one2one-rnn-BS128-LR0.05-Layer1-Dim150-Emb100-Dropout0.0-Copytrue-Covfalse-Contboth-IF1_step_30000.pt'\n",
    "one2seq_ckpt_path = '../models/keyphrase/meng17-one2seq-kp20k-topmodels/kp20k-meng17-verbatim_append-rnn-BS64-LR0.05-Layer1-Dim150-Emb100-Dropout0.0-Copytrue-Reusetrue-Covtrue-PEfalse-Contboth-IF1_step_50000.pt'\n",
    "opt = parser.parse_args('-config %s' % (config_path))\n",
    "setattr(opt, 'models', [one2one_ckpt_path])\n",
    "\n",
    "translator = build_translator(opt, report_score=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/memray/Project/anaconda3/lib/python3.6/site-packages/torchtext/data/field.py:359: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  var = torch.tensor(arr, dtype=self.dtype, device=device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Translating 10/1\n",
      "Total translation time (s): 0.421937\n",
      "Average translation time (s): 0.421937\n",
      "Tokens per second: 4.740044\n"
     ]
    }
   ],
   "source": [
    "scores, predictions = translator.translate(\n",
    "    src=[text_to_extract],\n",
    "    tgt=None,\n",
    "    src_dir=opt.src_dir,\n",
    "    batch_size=opt.batch_size,\n",
    "    attn_debug=opt.attn_debug,\n",
    "    opt=opt\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Paragraph:\n",
      "\tCSX was incorporated in 1978 under Virginia law. In 1980, the Company completed the merger of the Chessie System and Seaboard Coast Line Industries into CSX. The merger allowed the Company to connect northern population centers and Appalachian coal fields to growing southeastern markets. Later, the Company’s acquisition of key portions of Conrail, Inc. (\"Conrail\") allowed CSXT to link the northeast, including New England and the New York metropolitan area, with Chicago and midwestern markets as well as the growing areas in the Southeast already served by CSXT. This current rail network allows the Company to directly serve every major market in the eastern United States with safe, dependable, environmentally responsible and fuel efficient freight transportation and intermodal service.\n",
      "\n",
      "Top predictions:\n",
      "\t1: coal fields\n",
      "\t2: CSX\n",
      "\t3: 1978\n",
      "\t4: Virginia\n",
      "\t5: Company\n",
      "\t6: Chessie\n",
      "\t7: merger\n",
      "\t8: New\n",
      "\t9: CSX.\n",
      "\t10: freight transportation\n",
      "\t11: law.\n",
      "\t12: rail network\n",
      "\t13: Seaboard\n",
      "\t14: coal\n",
      "\t15: Company’s\n",
      "\t16: Conrail,\n",
      "\t17: Chicago\n",
      "\t18: Line\n",
      "\t19: Coast\n",
      "\t20: population centers\n"
     ]
    }
   ],
   "source": [
    "print('Paragraph:\\n\\t'+text_to_extract)\n",
    "print('Top predictions:')\n",
    "keyphrases = [kp.strip() for kp in predictions[0] if (not kp.lower().strip() in stoplist) and (kp != '<unk>' )]\n",
    "for kp_id, kp in enumerate(keyphrases[: min(len(keyphrases), 20)]):\n",
    "    print('\\t%d: %s' % (kp_id+1, kp))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PKE models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Initialize DF (only run once)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pke import compute_document_frequency\n",
    "from string import punctuation\n",
    "\n",
    "# stoplist for filtering n-grams\n",
    "stoplist=list(punctuation)\n",
    "\n",
    "# compute df counts and store as n-stem -> weight values\n",
    "compute_document_frequency(input_dir=data_root_path,\n",
    "                           output_file=data_root_path+'/10k.df.tsv.gz',\n",
    "                           extension='xml',           # input file extension\n",
    "                           language='en',                # language of files\n",
    "                           normalization=\"stemming\",    # use porter stemmer\n",
    "                           stoplist=stoplist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/memray/Project/anaconda3/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: DeprecationWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.\n",
      "  warnings.warn(msg, category=DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t1: csx (0.0000)\n",
      "\t2: csx was (0.0000)\n",
      "\t3: csx was incorporated (0.0000)\n",
      "\t4: was (0.0000)\n",
      "\t5: was incorporated (0.0000)\n",
      "\t6: was incorporated in (0.0000)\n",
      "\t7: incorporated (0.0000)\n",
      "\t8: incorporated in (0.0000)\n",
      "\t9: incorporated in 1978 (0.0000)\n",
      "\t10: in 1978 (0.0000)\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "import pke\n",
    "\n",
    "# 1. create a TfIdf extractor.\n",
    "extractor = pke.unsupervised.TfIdf()\n",
    "\n",
    "# 2. load the content of the document.\n",
    "extractor.load_document(input=text_to_extract,\n",
    "                        language='en_core_web_sm',\n",
    "                        normalization=None)\n",
    "\n",
    "# 3. select {1-3}-grams not containing punctuation marks as candidates.\n",
    "extractor.candidate_selection(n=3, stoplist=list(string.punctuation))\n",
    "\n",
    "# 4. weight the candidates using a `tf` x `idf`\n",
    "df = pke.load_document_frequency_file(input_file=data_root_path+'/10k.df.tsv.gz')\n",
    "extractor.candidate_weighting(df=df)\n",
    "\n",
    "# 5. get the 10-highest scored candidates as keyphrases\n",
    "keyphrases = extractor.get_n_best(n=10)\n",
    "for kp_id, kp in enumerate(keyphrases):\n",
    "    print('\\t%d: %s (%.4f)' % (kp_id+1, kp[0], kp[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### YAKE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t1: seaboard coast line (0.0068)\n",
      "\t2: coast line industries (0.0068)\n",
      "\t3: virginia law (0.0073)\n",
      "\t4: chessie system (0.0348)\n",
      "\t5: seaboard coast (0.0348)\n",
      "\t6: coast line (0.0348)\n",
      "\t7: line industries (0.0348)\n",
      "\t8: company (0.0410)\n",
      "\t9: including new england (0.0450)\n",
      "\t10: new york metropolitan (0.0496)\n"
     ]
    }
   ],
   "source": [
    "import pke\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# 1. create a YAKE extractor.\n",
    "extractor = pke.unsupervised.YAKE()\n",
    "\n",
    "# 2. load the content of the document.\n",
    "extractor.load_document(input=text_to_extract,\n",
    "                        language='en_core_web_sm',\n",
    "                        normalization=None)\n",
    "\n",
    "\n",
    "# 3. select {1-3}-grams not containing punctuation marks and not\n",
    "#    beginning/ending with a stopword as candidates.\n",
    "extractor.candidate_selection(n=3, stoplist=stoplist)\n",
    "\n",
    "# 4. weight the candidates using YAKE weighting scheme, a window (in\n",
    "#    words) for computing left/right contexts can be specified.\n",
    "window = 2\n",
    "use_stems = False # use stems instead of words for weighting\n",
    "extractor.candidate_weighting(window=window,\n",
    "                              stoplist=stoplist,\n",
    "                              use_stems=use_stems)\n",
    "\n",
    "# 5. get the 10-highest scored candidates as keyphrases.\n",
    "#    redundant keyphrases are removed from the output using levenshtein\n",
    "#    distance and a threshold.\n",
    "threshold = 0.8\n",
    "keyphrases = extractor.get_n_best(n=10, threshold=threshold)\n",
    "for kp_id, kp in enumerate(keyphrases):\n",
    "    print('\\t%d: %s (%.4f)' % (kp_id+1, kp[0], kp[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TextRank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Candidates are generated using 0.33-top\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t1: our data practices (0.1481)\n",
      "\t2: data protection laws (0.1409)\n",
      "\t3: data protection (0.1090)\n",
      "\t4: our business practices (0.0960)\n",
      "\t5: data (0.0796)\n",
      "\t6: manner adverse (0.0750)\n",
      "\t7: regulatory proposals (0.0750)\n",
      "\t8: our business (0.0685)\n",
      "\t9: substantial costs (0.0578)\n",
      "\t10: such interpretations (0.0578)\n"
     ]
    }
   ],
   "source": [
    "import pke\n",
    "\n",
    "# define the set of valid Part-of-Speeches\n",
    "pos = {'NOUN', 'PROPN', 'ADJ'}\n",
    "\n",
    "# 1. create a TextRank extractor.\n",
    "extractor = pke.unsupervised.TextRank()\n",
    "\n",
    "# 2. load the content of the document.\n",
    "extractor.load_document(input=text_to_extract,\n",
    "                        language='en_core_web_sm',\n",
    "                        normalization=None)\n",
    "\n",
    "# 3. build the graph representation of the document and rank the words.\n",
    "#    Keyphrase candidates are composed from the 33-percent\n",
    "#    highest-ranked words.\n",
    "extractor.candidate_weighting(window=2,\n",
    "                              pos=pos,\n",
    "                              top_percent=0.33)\n",
    "\n",
    "# 4. get the 10-highest scored candidates as keyphrases\n",
    "keyphrases = extractor.get_n_best(n=10)\n",
    "for kp_id, kp in enumerate(keyphrases):\n",
    "    print('\\t%d: %s (%.4f)' % (kp_id+1, kp[0], kp[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
