{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rui.meng/anaconda3/lib/python3.7/site-packages/sklearn/externals/joblib/__init__.py:15: DeprecationWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.\n",
      "  warnings.warn(msg, category=DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "exec('from __future__ import unicode_literals')\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import random\n",
    "\n",
    "module_path = os.path.abspath(os.path.join('../'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "module_path = os.path.abspath(os.path.join('../onmt'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "\n",
    "from itertools import repeat\n",
    "\n",
    "from onmt.utils.logging import init_logger\n",
    "from onmt.utils.misc import split_corpus\n",
    "from onmt.translate.translator import build_translator\n",
    "\n",
    "import onmt.opts as opts\n",
    "from onmt.utils.parse import ArgumentParser\n",
    "from kp_gen_eval import _get_parser\n",
    "\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "stoplist = stopwords.words('english')\n",
    "\n",
    "from string import punctuation\n",
    "import onmt.keyphrase.pke as pke\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "import onmt.keyphrase.kp_inference as kp_inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'onmt.keyphrase.kp_inference' from '/Users/rui.meng/project/OpenNMT-kpg/onmt/keyphrase/kp_inference.py'>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import importlib\n",
    "importlib.reload(kp_inference)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load a paragraph from 10-K docs (assume current directory is OpenNMT-kpg/notebook/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/memray/project/kp/OpenNMT-kpg/data/salesforce/10k\n",
      "Loaded #(10k docs)=11252\n"
     ]
    }
   ],
   "source": [
    "data_root_path = '../data/salesforce/10k'\n",
    "print(os.path.abspath(data_root_path))\n",
    "doc_dicts = []\n",
    "for subdir, dirs, files in os.walk(data_root_path):\n",
    "    for file in files:\n",
    "        filepath = subdir + os.sep + file\n",
    "        if filepath.endswith(\".txt\"):\n",
    "#             print(filepath)\n",
    "            text = open(filepath, 'r').readlines()\n",
    "            pars = [l for l in text if len(l.split()) > 20]\n",
    "            doc = {'name': file, 'path': filepath, 'text': text, 'paragraph': pars}\n",
    "#             print('#line=%d, #par=%d' % (len(text), len(pars)))\n",
    "            doc_dicts.append(doc)\n",
    "    \n",
    "print('Loaded #(10k docs)=%d' % (len(doc_dicts)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Sample a paragraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7705\n",
      "3\n",
      "CSX was incorporated in 1978 under Virginia law. In 1980, the Company completed the merger of the Chessie System and Seaboard Coast Line Industries into CSX. The merger allowed the Company to connect northern population centers and Appalachian coal fields to growing southeastern markets. Later, the Company’s acquisition of key portions of Conrail, Inc. (\"Conrail\") allowed CSXT to link the northeast, including New England and the New York metropolitan area, with Chicago and midwestern markets as well as the growing areas in the Southeast already served by CSXT. This current rail network allows the Company to directly serve every major market in the eastern United States with safe, dependable, environmentally responsible and fuel efficient freight transportation and intermodal service.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "doc_id = random.randint(0, len(doc_dicts))\n",
    "doc = doc_dicts[doc_id]\n",
    "pars = doc['paragraph']\n",
    "par_id = random.randint(0, len(doc_dicts[doc_id]['paragraph']))\n",
    "text_to_extract = pars[par_id]\n",
    "print(doc_id)\n",
    "print(par_id)\n",
    "print(text_to_extract)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Or load a PROD description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded #(PROD docs)=20\n",
      "17\n",
      "Risk\n",
      "Anything that threatens a company's ability to meet its target or achieve its financial goals is called business risk\n",
      "These risks come from a variety of sources, so it's not always the company head or a manager who's to blame\n",
      "Instead, the risks may come from other sources within the firm or they may be external—from regulations to the overall economy\n",
      "While a company may not be able to shelter itself from risk completely, there are ways it can help protect itself from the effects of business risk, primarily by adopting a risk management strategy\n"
     ]
    }
   ],
   "source": [
    "PROD_path = '../data/salesforce/SF_Prod'\n",
    "\n",
    "prod_dicts = []\n",
    "for subdir, dirs, files in os.walk(PROD_path):\n",
    "    for file in files:\n",
    "        filepath = subdir + os.sep + file\n",
    "        text = open(filepath, 'r').readlines()\n",
    "        text = '\\n'.join([l.strip() for l in text])\n",
    "        doc = {'name': file, 'path': filepath, 'text': text}\n",
    "        prod_dicts.append(doc)\n",
    "\n",
    "print('Loaded #(PROD docs)=%d' % (len(prod_dicts)))\n",
    "\n",
    "doc_id = random.randint(0, len(prod_dicts))\n",
    "doc = prod_dicts[doc_id]\n",
    "text_to_extract = doc['text']\n",
    "print(doc_id)\n",
    "print(doc['name'])\n",
    "print(text_to_extract)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Supervised Deep Keyphrase Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = _get_parser()\n",
    "config_path = '../config/translate/config-rnn-keyphrase.yml'\n",
    "one2one_ckpt_path = '../models/keyphrase/meng17-one2one-kp20k-topmodels/kp20k-meng17-one2one-rnn-BS128-LR0.05-Layer1-Dim150-Emb100-Dropout0.0-Copytrue-Covfalse-Contboth-IF1_step_30000.pt'\n",
    "one2seq_ckpt_path = '../models/keyphrase/meng17-one2seq-kp20k-topmodels/kp20k-meng17-verbatim_append-rnn-BS64-LR0.05-Layer1-Dim150-Emb100-Dropout0.0-Copytrue-Reusetrue-Covtrue-PEfalse-Contboth-IF1_step_50000.pt'\n",
    "opt = parser.parse_args('-config %s' % (config_path))\n",
    "setattr(opt, 'models', [one2one_ckpt_path])\n",
    "\n",
    "translator = build_translator(opt, report_score=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rui.meng/anaconda3/lib/python3.7/site-packages/torchtext/data/field.py:359: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  var = torch.tensor(arr, dtype=self.dtype, device=device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Translating 10/1\n",
      "Total translation time (s): 0.284127\n",
      "Average translation time (s): 0.284127\n",
      "Tokens per second: 7.039106\n",
      "Paragraph:\n",
      "\tAnything that threatens a company's ability to meet its target or achieve its financial goals is called business risk\n",
      "These risks come from a variety of sources, so it's not always the company head or a manager who's to blame\n",
      "Instead, the risks may come from other sources within the firm or they may be external—from regulations to the overall economy\n",
      "While a company may not be able to shelter itself from risk completely, there are ways it can help protect itself from the effects of business risk, primarily by adopting a risk management strategy\n",
      "Top predictions:\n",
      "\t1: risk management\n",
      "\t2: business risk\n",
      "\t3: Anything\n",
      "\t4: financial goals\n",
      "\t5: business risk management\n",
      "\t6: risk\n",
      "\t7: company's\n",
      "\t8: business risk,\n",
      "\t9: business\n",
      "\t10: blame\n",
      "\t11: goals\n",
      "\t12: risk management strategy\n",
      "\t13: economy\n",
      "\t14: sources,\n",
      "\t15: risk,\n",
      "\t16: management\n",
      "\t17: completely,\n",
      "\t18: business risk These\n",
      "\t19: risks\n",
      "\t20: risk analysis\n"
     ]
    }
   ],
   "source": [
    "scores, predictions = translator.translate(\n",
    "    src=[text_to_extract],\n",
    "    tgt=None,\n",
    "    src_dir=opt.src_dir,\n",
    "    batch_size=opt.batch_size,\n",
    "    attn_debug=opt.attn_debug,\n",
    "    opt=opt\n",
    ")\n",
    "print('Paragraph:\\n\\t'+text_to_extract)\n",
    "print('Top predictions:')\n",
    "keyphrases = [kp.strip() for kp in predictions[0] if (not kp.lower().strip() in stoplist) and (kp != '<unk>' )]\n",
    "for kp_id, kp in enumerate(keyphrases[: min(len(keyphrases), 20)]):\n",
    "    print('\\t%d: %s' % (kp_id+1, kp))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PKE models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t1: anything (0.0000)\n",
      "\t2: anything that (0.0000)\n",
      "\t3: anything that threatens (0.0000)\n",
      "\t4: that (0.0000)\n",
      "\t5: that threatens (0.0000)\n",
      "\t6: threatens (0.0000)\n",
      "\t7: company (0.0000)\n",
      "\t8: ability (0.0000)\n",
      "\t9: ability to (0.0000)\n",
      "\t10: ability to meet (0.0000)\n",
      "\t11: to meet (0.0000)\n",
      "\t12: to meet its (0.0000)\n",
      "\t13: meet (0.0000)\n",
      "\t14: meet its (0.0000)\n",
      "\t15: meet its target (0.0000)\n",
      "\t16: its (0.0000)\n",
      "\t17: its target (0.0000)\n",
      "\t18: its target or (0.0000)\n",
      "\t19: target (0.0000)\n",
      "\t20: target or (0.0000)\n"
     ]
    }
   ],
   "source": [
    "dataset_name = 'SF_Prod'\n",
    "dataset_path = '../data/salesforce/%s/' % dataset_name\n",
    "_ = kp_inference.extract_pke(text_to_extract, method='tfidf' , dataset_path=dataset_path,\n",
    "            df_path=os.path.abspath(dataset_path + '../%s.df.tsv.gz' % dataset_name), top_k=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### YAKE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t1: called business risk (0.0331)\n",
      "\t2: financial goals (0.0332)\n",
      "\t3: called business (0.0632)\n",
      "\t4: business risk (0.0830)\n",
      "\t5: risk (0.1075)\n",
      "\t6: company (0.1209)\n",
      "\t7: anything (0.1383)\n",
      "\t8: ability (0.1383)\n",
      "\t9: risks may come (0.1415)\n",
      "\t10: business (0.1657)\n",
      "\t11: may (0.1765)\n",
      "\t12: threatens (0.1793)\n",
      "\t13: meet (0.1793)\n",
      "\t14: target (0.1793)\n",
      "\t15: achieve (0.1793)\n",
      "\t16: financial (0.1793)\n",
      "\t17: goals (0.1793)\n",
      "\t18: called (0.1793)\n",
      "\t19: risk management strategy (0.1844)\n",
      "\t20: anything that threatens (0.1851)\n"
     ]
    }
   ],
   "source": [
    "_ = kp_inference.extract_pke(text_to_extract, method='yake', top_k=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TextRank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Candidates are generated using 0.33-top\n",
      "WARNING:root:Not enough candidates to choose from (10 requested, 6 given)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t1: risk management (0.1901)\n",
      "\t2: company head (0.1464)\n",
      "\t3: financial goals (0.1464)\n",
      "\t4: risk (0.0950)\n",
      "\t5: sources (0.0732)\n",
      "\t6: company (0.0732)\n"
     ]
    }
   ],
   "source": [
    "# define the set of valid Part-of-Speeches\n",
    "pos = {'NOUN', 'PROPN', 'ADJ'}\n",
    "\n",
    "# 1. create a TextRank extractor.\n",
    "extractor = pke.unsupervised.TextRank()\n",
    "\n",
    "# 2. load the content of the document.\n",
    "extractor.load_document(input=text_to_extract,\n",
    "                        language='en_core_web_sm',\n",
    "                        normalization=None)\n",
    "\n",
    "# 3. build the graph representation of the document and rank the words.\n",
    "#    Keyphrase candidates are composed from the 33-percent\n",
    "#    highest-ranked words.\n",
    "extractor.candidate_weighting(window=2,\n",
    "                              pos=pos,\n",
    "                              top_percent=0.33)\n",
    "\n",
    "# 4. get the 10-highest scored candidates as keyphrases\n",
    "keyphrases = extractor.get_n_best(n=10)\n",
    "for kp_id, kp in enumerate(keyphrases):\n",
    "    print('\\t%d: %s (%.4f)' % (kp_id+1, kp[0], kp[1]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
