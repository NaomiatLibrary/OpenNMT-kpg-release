{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exec('from __future__ import unicode_literals')\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import random\n",
    "\n",
    "module_path = os.path.abspath(os.path.join('../'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "module_path = os.path.abspath(os.path.join('../onmt'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "\n",
    "from itertools import repeat\n",
    "\n",
    "from onmt.utils.logging import init_logger\n",
    "from onmt.utils.misc import split_corpus\n",
    "from onmt.translate.translator import build_translator\n",
    "\n",
    "import onmt.opts as opts\n",
    "from onmt.utils.parse import ArgumentParser\n",
    "from kp_gen_eval import _get_parser\n",
    "\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "stoplist = stopwords.words('english')\n",
    "\n",
    "from string import punctuation\n",
    "import onmt.keyphrase.pke as pke\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "import onmt.keyphrase.kp_inference as kp_inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'onmt.keyphrase.kp_inference' from '/Users/memray/project/kp/OpenNMT-kpg/onmt/keyphrase/kp_inference.py'>"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import importlib\n",
    "importlib.reload(kp_inference)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load a paragraph from 10-K docs (assume current directory is OpenNMT-kpg/notebook/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/memray/project/kp/OpenNMT-kpg/data/salesforce/10k\n",
      "Loaded #(10k docs)=11252\n"
     ]
    }
   ],
   "source": [
    "data_root_path = '../data/salesforce/10k'\n",
    "print(os.path.abspath(data_root_path))\n",
    "doc_dicts = []\n",
    "for subdir, dirs, files in os.walk(data_root_path):\n",
    "    for file in files:\n",
    "        filepath = subdir + os.sep + file\n",
    "        if filepath.endswith(\".txt\"):\n",
    "#             print(filepath)\n",
    "            text = open(filepath, 'r').readlines()\n",
    "            pars = [l for l in text if len(l.split()) > 20]\n",
    "            doc = {'name': file, 'path': filepath, 'text': text, 'paragraph': pars}\n",
    "#             print('#line=%d, #par=%d' % (len(text), len(pars)))\n",
    "            doc_dicts.append(doc)\n",
    "    \n",
    "print('Loaded #(10k docs)=%d' % (len(doc_dicts)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Sample a paragraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7705\n",
      "3\n",
      "CSX was incorporated in 1978 under Virginia law. In 1980, the Company completed the merger of the Chessie System and Seaboard Coast Line Industries into CSX. The merger allowed the Company to connect northern population centers and Appalachian coal fields to growing southeastern markets. Later, the Companyâ€™s acquisition of key portions of Conrail, Inc. (\"Conrail\") allowed CSXT to link the northeast, including New England and the New York metropolitan area, with Chicago and midwestern markets as well as the growing areas in the Southeast already served by CSXT. This current rail network allows the Company to directly serve every major market in the eastern United States with safe, dependable, environmentally responsible and fuel efficient freight transportation and intermodal service.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "doc_id = random.randint(0, len(doc_dicts))\n",
    "doc = doc_dicts[doc_id]\n",
    "pars = doc['paragraph']\n",
    "par_id = random.randint(0, len(doc_dicts[doc_id]['paragraph']))\n",
    "text_to_extract = pars[par_id]\n",
    "print(doc_id)\n",
    "print(par_id)\n",
    "print(text_to_extract)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-2a5a2f6deefa>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mdoc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdoc_dicts\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdoc_id\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mpars\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdoc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'paragraph'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mpar\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpars\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpar_id\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpar_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "# doc_id = 9539\n",
    "# par_id = 67\n",
    "doc = doc_dicts[doc_id]\n",
    "pars = doc['paragraph']\n",
    "par = pars[par_id]\n",
    "print(doc_id)\n",
    "print(par_id)\n",
    "print(par)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Or load a PROD description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded #(PROD docs)=20\n",
      "15\n",
      "Sales_einstein\n",
      "an artificial intelligence built into the core of the salesforce platform\n",
      "it helps sales representatives win more deals with intelligent automation smarter scoring actionable insights and predictive forecasting\n",
      "increases sales productivity at every step of the sales process with key predictions intelligent recommendations and timely automation\n",
      "it is a data science department that learns from your teams sales activities and crm data and helps you identify the best leads convert opportunities more efficiently and retain customers with ease\n",
      "it also includes the sales analytics app and inbox\n"
     ]
    }
   ],
   "source": [
    "PROD_path = '../data/salesforce/SF_Prod'\n",
    "\n",
    "prod_dicts = []\n",
    "for subdir, dirs, files in os.walk(PROD_path):\n",
    "    for file in files:\n",
    "        filepath = subdir + os.sep + file\n",
    "        text = open(filepath, 'r').readlines()\n",
    "        text = '\\n'.join([l.strip() for l in text])\n",
    "        doc = {'name': file, 'path': filepath, 'text': text}\n",
    "        prod_dicts.append(doc)\n",
    "\n",
    "print('Loaded #(PROD docs)=%d' % (len(prod_dicts)))\n",
    "\n",
    "doc_id = random.randint(0, len(prod_dicts))\n",
    "doc = prod_dicts[doc_id]\n",
    "text_to_extract = doc['text']\n",
    "print(doc_id)\n",
    "print(doc['name'])\n",
    "print(text_to_extract)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Supervised Deep Keyphrase Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = _get_parser()\n",
    "config_path = '../config/translate/config-rnn-keyphrase.yml'\n",
    "one2one_ckpt_path = '../models/keyphrase/meng17-one2one-kp20k-topmodels/kp20k-meng17-one2one-rnn-BS128-LR0.05-Layer1-Dim150-Emb100-Dropout0.0-Copytrue-Covfalse-Contboth-IF1_step_30000.pt'\n",
    "one2seq_ckpt_path = '../models/keyphrase/meng17-one2seq-kp20k-topmodels/kp20k-meng17-verbatim_append-rnn-BS64-LR0.05-Layer1-Dim150-Emb100-Dropout0.0-Copytrue-Reusetrue-Covtrue-PEfalse-Contboth-IF1_step_50000.pt'\n",
    "opt = parser.parse_args('-config %s' % (config_path))\n",
    "setattr(opt, 'models', [one2one_ckpt_path])\n",
    "\n",
    "translator = build_translator(opt, report_score=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Translating 10/1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/memray/Project/anaconda3/lib/python3.6/site-packages/torchtext/data/field.py:359: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  var = torch.tensor(arr, dtype=self.dtype, device=device)\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Expected object of scalar type Byte but got scalar type Bool for argument #3 'other' in call to _th_ior_",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-41-ec06f2694117>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mopt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mattn_debug\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mopt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mattn_debug\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0mopt\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mopt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m )\n",
      "\u001b[0;32m/Users/memray/project/kp/OpenNMT-kpg/onmt/translate/translator.py\u001b[0m in \u001b[0;36mtranslate\u001b[0;34m(self, src, tgt, src_dir, batch_size, attn_debug, phrase_table, opt)\u001b[0m\n\u001b[1;32m    370\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    371\u001b[0m             batch_data = self.translate_batch(\n\u001b[0;32m--> 372\u001b[0;31m                 \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msrc_vocabs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattn_debug\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    373\u001b[0m             )\n\u001b[1;32m    374\u001b[0m             \u001b[0mtranslations\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mxlation_builder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/memray/project/kp/OpenNMT-kpg/onmt/translate/translator.py\u001b[0m in \u001b[0;36mtranslate_batch\u001b[0;34m(self, batch, src_vocabs, attn_debug)\u001b[0m\n\u001b[1;32m    593\u001b[0m                     \u001b[0mratio\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mratio\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    594\u001b[0m                     \u001b[0mn_best\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_best\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 595\u001b[0;31m                     return_attention=attn_debug or self.replace_unk)\n\u001b[0m\u001b[1;32m    596\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    597\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_run_encoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/memray/project/kp/OpenNMT-kpg/onmt/translate/translator.py\u001b[0m in \u001b[0;36m_translate_batch\u001b[0;34m(self, batch, src_vocabs, max_length, min_length, ratio, n_best, return_attention)\u001b[0m\n\u001b[1;32m    766\u001b[0m             \u001b[0many_beam_is_finished\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbeam\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_finished\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0many\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    767\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0many_beam_is_finished\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 768\u001b[0;31m                 \u001b[0mbeam\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate_finished\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlast_step\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0mmax_length\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    769\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mbeam\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    770\u001b[0m                     \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/memray/project/kp/OpenNMT-kpg/onmt/translate/beam_search.py\u001b[0m in \u001b[0;36mupdate_finished\u001b[0;34m(self, last_step)\u001b[0m\n\u001b[1;32m    215\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_finished\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_finished\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'cpu'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    216\u001b[0m         \u001b[0;31m# TODO @memray: extend to topK beam finished?\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 217\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtop_beam_finished\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtop_beam_finished\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_finished\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meq\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    218\u001b[0m         \u001b[0mpredictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0malive_seq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_B_old\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbeam_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    219\u001b[0m         attention = (\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Expected object of scalar type Byte but got scalar type Bool for argument #3 'other' in call to _th_ior_"
     ]
    }
   ],
   "source": [
    "scores, predictions = translator.translate(\n",
    "    src=[text_to_extract],\n",
    "    tgt=None,\n",
    "    src_dir=opt.src_dir,\n",
    "    batch_size=opt.batch_size,\n",
    "    attn_debug=opt.attn_debug,\n",
    "    opt=opt\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Paragraph:\n",
      "\tCSX was incorporated in 1978 under Virginia law. In 1980, the Company completed the merger of the Chessie System and Seaboard Coast Line Industries into CSX. The merger allowed the Company to connect northern population centers and Appalachian coal fields to growing southeastern markets. Later, the Companyâ€™s acquisition of key portions of Conrail, Inc. (\"Conrail\") allowed CSXT to link the northeast, including New England and the New York metropolitan area, with Chicago and midwestern markets as well as the growing areas in the Southeast already served by CSXT. This current rail network allows the Company to directly serve every major market in the eastern United States with safe, dependable, environmentally responsible and fuel efficient freight transportation and intermodal service.\n",
      "\n",
      "Top predictions:\n",
      "\t1: coal fields\n",
      "\t2: CSX\n",
      "\t3: 1978\n",
      "\t4: Virginia\n",
      "\t5: Company\n",
      "\t6: Chessie\n",
      "\t7: merger\n",
      "\t8: New\n",
      "\t9: CSX.\n",
      "\t10: freight transportation\n",
      "\t11: law.\n",
      "\t12: rail network\n",
      "\t13: Seaboard\n",
      "\t14: coal\n",
      "\t15: Companyâ€™s\n",
      "\t16: Conrail,\n",
      "\t17: Chicago\n",
      "\t18: Line\n",
      "\t19: Coast\n",
      "\t20: population centers\n"
     ]
    }
   ],
   "source": [
    "print('Paragraph:\\n\\t'+text_to_extract)\n",
    "print('Top predictions:')\n",
    "keyphrases = [kp.strip() for kp in predictions[0] if (not kp.lower().strip() in stoplist) and (kp != '<unk>' )]\n",
    "for kp_id, kp in enumerate(keyphrases[: min(len(keyphrases), 20)]):\n",
    "    print('\\t%d: %s' % (kp_id+1, kp))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PKE models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t1: salesforce (0.0000)\n",
      "\t2: salesforce built (0.0000)\n",
      "\t3: salesforce built intelligence (0.0000)\n",
      "\t4: built (0.0000)\n",
      "\t5: built intelligence (0.0000)\n",
      "\t6: built intelligence artificial (0.0000)\n",
      "\t7: intelligence (0.0000)\n",
      "\t8: intelligence artificial (0.0000)\n",
      "\t9: intelligence artificial core (0.0000)\n",
      "\t10: artificial (0.0000)\n",
      "\t11: artificial core (0.0000)\n",
      "\t12: artificial core platform (0.0000)\n",
      "\t13: core (0.0000)\n",
      "\t14: core platform (0.0000)\n",
      "\t15: platform (0.0000)\n",
      "\t16: predictive (0.0000)\n",
      "\t17: predictive forecasting (0.0000)\n",
      "\t18: predictive forecasting more (0.0000)\n",
      "\t19: forecasting (0.0000)\n",
      "\t20: forecasting more (0.0000)\n"
     ]
    }
   ],
   "source": [
    "dataset_name = 'SF_Prod'\n",
    "dataset_path = '../data/salesforce/%s/' % dataset_name\n",
    "_ = kp_inference.extract_pke(text_to_extract, method='tfidf' , dataset_path=dataset_path,\n",
    "            df_path=os.path.abspath(dataset_path + '../%s.df.tsv.gz' % dataset_name), top_k=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### YAKE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t1: artificial intelligence built (0.0009)\n",
      "\t2: smarter scoring actionable (0.0009)\n",
      "\t3: scoring actionable insights (0.0009)\n",
      "\t4: best leads convert (0.0009)\n",
      "\t5: leads convert opportunities (0.0009)\n",
      "\t6: automation smarter scoring (0.0011)\n",
      "\t7: key predictions intelligent (0.0011)\n",
      "\t8: predictions intelligent recommendations (0.0011)\n",
      "\t9: data science department (0.0011)\n",
      "\t10: increases sales productivity (0.0011)\n",
      "\t11: intelligent automation smarter (0.0012)\n",
      "\t12: sales representatives win (0.0014)\n",
      "\t13: teams sales activities (0.0014)\n",
      "\t14: sales analytics app (0.0014)\n",
      "\t15: helps sales representatives (0.0015)\n",
      "\t16: salesforce platform (0.0074)\n",
      "\t17: predictive forecasting (0.0074)\n",
      "\t18: artificial intelligence (0.0094)\n",
      "\t19: intelligence built (0.0094)\n",
      "\t20: representatives win (0.0094)\n"
     ]
    }
   ],
   "source": [
    "_ = kp_inference.extract_pke(text_to_extract, method='yake', top_k=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TextRank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Candidates are generated using 0.33-top\n",
      "WARNING:root:Not enough candidates to choose from (10 requested, 8 given)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t1: intelligent automation timely (0.1305)\n",
      "\t2: automation intelligent insights (0.1305)\n",
      "\t3: artificial core (0.0776)\n",
      "\t4: data (0.0539)\n",
      "\t5: analytics (0.0434)\n",
      "\t6: predictions (0.0434)\n",
      "\t7: forecasting (0.0434)\n",
      "\t8: convert (0.0329)\n"
     ]
    }
   ],
   "source": [
    "# define the set of valid Part-of-Speeches\n",
    "pos = {'NOUN', 'PROPN', 'ADJ'}\n",
    "\n",
    "# 1. create a TextRank extractor.\n",
    "extractor = pke.unsupervised.TextRank()\n",
    "\n",
    "# 2. load the content of the document.\n",
    "extractor.load_document(input=text_to_extract,\n",
    "                        language='en_core_web_sm',\n",
    "                        normalization=None)\n",
    "\n",
    "# 3. build the graph representation of the document and rank the words.\n",
    "#    Keyphrase candidates are composed from the 33-percent\n",
    "#    highest-ranked words.\n",
    "extractor.candidate_weighting(window=2,\n",
    "                              pos=pos,\n",
    "                              top_percent=0.33)\n",
    "\n",
    "# 4. get the 10-highest scored candidates as keyphrases\n",
    "keyphrases = extractor.get_n_best(n=10)\n",
    "for kp_id, kp in enumerate(keyphrases):\n",
    "    print('\\t%d: %s (%.4f)' % (kp_id+1, kp[0], kp[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
