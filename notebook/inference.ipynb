{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/memray/Project/anaconda3/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: DeprecationWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.\n",
      "  warnings.warn(msg, category=DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "exec('from __future__ import unicode_literals')\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import random\n",
    "\n",
    "module_path = os.path.abspath(os.path.join('../'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "module_path = os.path.abspath(os.path.join('../onmt'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "\n",
    "from itertools import repeat\n",
    "\n",
    "from onmt.utils.logging import init_logger\n",
    "from onmt.utils.misc import split_corpus\n",
    "from onmt.translate.translator import build_translator\n",
    "\n",
    "import onmt.opts as opts\n",
    "from onmt.utils.parse import ArgumentParser\n",
    "from kp_gen_eval import _get_parser\n",
    "\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "stoplist = stopwords.words('english')\n",
    "\n",
    "from string import punctuation\n",
    "import onmt.keyphrase.pke as pke\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "import onmt.keyphrase.kp_inference as kp_inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'onmt.keyphrase.kp_inference' from '/Users/memray/project/kp/OpenNMT-kpg/onmt/keyphrase/kp_inference.py'>"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import importlib\n",
    "importlib.reload(kp_inference)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load a paragraph from 10-K docs (assume current directory is OpenNMT-kpg/notebook/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/memray/project/kp/OpenNMT-kpg/data/salesforce/10k\n",
      "Loaded #(10k docs)=11252\n"
     ]
    }
   ],
   "source": [
    "data_root_path = '../data/salesforce/10k'\n",
    "print(os.path.abspath(data_root_path))\n",
    "doc_dicts = []\n",
    "for subdir, dirs, files in os.walk(data_root_path):\n",
    "    for file in files:\n",
    "        filepath = subdir + os.sep + file\n",
    "        if filepath.endswith(\".txt\"):\n",
    "#             print(filepath)\n",
    "            text = open(filepath, 'r').readlines()\n",
    "            pars = [l for l in text if len(l.split()) > 20]\n",
    "            doc = {'name': file, 'path': filepath, 'text': text, 'paragraph': pars}\n",
    "#             print('#line=%d, #par=%d' % (len(text), len(pars)))\n",
    "            doc_dicts.append(doc)\n",
    "    \n",
    "print('Loaded #(10k docs)=%d' % (len(doc_dicts)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Sample a paragraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7705\n",
      "3\n",
      "CSX was incorporated in 1978 under Virginia law. In 1980, the Company completed the merger of the Chessie System and Seaboard Coast Line Industries into CSX. The merger allowed the Company to connect northern population centers and Appalachian coal fields to growing southeastern markets. Later, the Companyâ€™s acquisition of key portions of Conrail, Inc. (\"Conrail\") allowed CSXT to link the northeast, including New England and the New York metropolitan area, with Chicago and midwestern markets as well as the growing areas in the Southeast already served by CSXT. This current rail network allows the Company to directly serve every major market in the eastern United States with safe, dependable, environmentally responsible and fuel efficient freight transportation and intermodal service.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "doc_id = random.randint(0, len(doc_dicts))\n",
    "doc = doc_dicts[doc_id]\n",
    "pars = doc['paragraph']\n",
    "par_id = random.randint(0, len(doc_dicts[doc_id]['paragraph']))\n",
    "text_to_extract = pars[par_id]\n",
    "print(doc_id)\n",
    "print(par_id)\n",
    "print(text_to_extract)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Or load a PROD description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded #(PROD docs)=20\n",
      "2\n",
      "Marketing_norm\n",
      "enables and know across personalize engage analyze\n",
      "Deliver experiences personalized step campaign every lifecycle customer\n",
      "and manage experiences customer management driving engagement valuable interaction right real-time\n",
      "and transparent create customer valuable reach Expand marketers 2nd sharing data share and activate data Capture and any source audiences platform publishers then party can data trusted\n",
      "Use data build department every email smarter campaigns marketing sophisticated basic\n",
      "and Send consistent SMS, chat messages\n",
      "and engage create customer\n",
      "Connect artificial social and service powered platform\n",
      "actionable reports drive and Enable intelligence marketing data unifying AI-powered visualizing creating\n",
      "and partners sell consumers and Enable\n",
      "and known data Connect across sources and gain view unified devices multiple unknown\n",
      "Create business experiences customer seamless connecting and channels stage every lifecycle\n",
      "and return maximize impact and and Measure optimize performance marketing total investment drives\n"
     ]
    }
   ],
   "source": [
    "PROD_path = '../data/salesforce/SF_Prod'\n",
    "\n",
    "prod_dicts = []\n",
    "for subdir, dirs, files in os.walk(PROD_path):\n",
    "    for file in files:\n",
    "        filepath = subdir + os.sep + file\n",
    "        text = open(filepath, 'r').readlines()\n",
    "        text = '\\n'.join([l.strip() for l in text])\n",
    "        doc = {'name': file, 'path': filepath, 'text': text}\n",
    "        prod_dicts.append(doc)\n",
    "\n",
    "print('Loaded #(PROD docs)=%d' % (len(prod_dicts)))\n",
    "\n",
    "doc_id = random.randint(0, len(prod_dicts))\n",
    "doc = prod_dicts[doc_id]\n",
    "text_to_extract = doc['text']\n",
    "print(doc_id)\n",
    "print(doc['name'])\n",
    "print(text_to_extract)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Supervised Deep Keyphrase Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = _get_parser()\n",
    "config_path = '../config/translate/config-rnn-keyphrase.yml'\n",
    "one2one_ckpt_path = '../models/keyphrase/meng17-one2one-kp20k-topmodels/kp20k-meng17-one2one-rnn-BS128-LR0.05-Layer1-Dim150-Emb100-Dropout0.0-Copytrue-Covfalse-Contboth-IF1_step_30000.pt'\n",
    "one2seq_ckpt_path = '../models/keyphrase/meng17-one2seq-kp20k-topmodels/kp20k-meng17-verbatim_append-rnn-BS64-LR0.05-Layer1-Dim150-Emb100-Dropout0.0-Copytrue-Reusetrue-Covtrue-PEfalse-Contboth-IF1_step_50000.pt'\n",
    "opt = parser.parse_args('-config %s' % (config_path))\n",
    "setattr(opt, 'models', [one2one_ckpt_path])\n",
    "\n",
    "translator = build_translator(opt, report_score=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/memray/Project/anaconda3/lib/python3.6/site-packages/torchtext/data/field.py:359: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  var = torch.tensor(arr, dtype=self.dtype, device=device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Translating 10/1\n",
      "Total translation time (s): 0.344692\n",
      "Average translation time (s): 0.344692\n",
      "Tokens per second: 5.802278\n",
      "Paragraph:\n",
      "\tenables and know across personalize engage analyze\n",
      "Deliver experiences personalized step campaign every lifecycle customer\n",
      "and manage experiences customer management driving engagement valuable interaction right real-time\n",
      "and transparent create customer valuable reach Expand marketers 2nd sharing data share and activate data Capture and any source audiences platform publishers then party can data trusted\n",
      "Use data build department every email smarter campaigns marketing sophisticated basic\n",
      "and Send consistent SMS, chat messages\n",
      "and engage create customer\n",
      "Connect artificial social and service powered platform\n",
      "actionable reports drive and Enable intelligence marketing data unifying AI-powered visualizing creating\n",
      "and partners sell consumers and Enable\n",
      "and known data Connect across sources and gain view unified devices multiple unknown\n",
      "Create business experiences customer seamless connecting and channels stage every lifecycle\n",
      "and return maximize impact and and Measure optimize performance marketing total investment drives\n",
      "Top predictions:\n",
      "\t1: customer management\n",
      "\t2: marketing\n",
      "\t3: performance\n",
      "\t4: performance marketing\n",
      "\t5: engagement\n",
      "\t6: Deliver\n",
      "\t7: interaction\n",
      "\t8: step campaign\n",
      "\t9: Connect\n",
      "\t10: data Connect\n",
      "\t11: Enable\n",
      "\t12: investment drives\n",
      "\t13: campaign\n",
      "\t14: email\n",
      "\t15: investment\n",
      "\t16: customer Connect\n",
      "\t17: chat\n",
      "\t18: social and service\n",
      "\t19: Create\n",
      "\t20: lifecycle customer\n"
     ]
    }
   ],
   "source": [
    "scores, predictions = translator.translate(\n",
    "    src=[text_to_extract],\n",
    "    tgt=None,\n",
    "    src_dir=opt.src_dir,\n",
    "    batch_size=opt.batch_size,\n",
    "    attn_debug=opt.attn_debug,\n",
    "    opt=opt\n",
    ")\n",
    "print('Paragraph:\\n\\t'+text_to_extract)\n",
    "print('Top predictions:')\n",
    "keyphrases = [kp.strip() for kp in predictions[0] if (not kp.lower().strip() in stoplist) and (kp != '<unk>' )]\n",
    "for kp_id, kp in enumerate(keyphrases[: min(len(keyphrases), 20)]):\n",
    "    print('\\t%d: %s' % (kp_id+1, kp))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PKE models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t1: enables (0.0000)\n",
      "\t2: enables and (0.0000)\n",
      "\t3: enables and know (0.0000)\n",
      "\t4: and (0.0000)\n",
      "\t5: and know (0.0000)\n",
      "\t6: and know across (0.0000)\n",
      "\t7: know (0.0000)\n",
      "\t8: know across (0.0000)\n",
      "\t9: know across personalize (0.0000)\n",
      "\t10: across (0.0000)\n",
      "\t11: across personalize (0.0000)\n",
      "\t12: across personalize engage (0.0000)\n",
      "\t13: personalize (0.0000)\n",
      "\t14: personalize engage (0.0000)\n",
      "\t15: personalize engage analyze (0.0000)\n",
      "\t16: engage (0.0000)\n",
      "\t17: engage analyze (0.0000)\n",
      "\t18: analyze (0.0000)\n",
      "\t19: deliver (0.0000)\n",
      "\t20: deliver experiences (0.0000)\n"
     ]
    }
   ],
   "source": [
    "dataset_name = 'SF_Prod'\n",
    "dataset_path = '../data/salesforce/%s/' % dataset_name\n",
    "_ = kp_inference.extract_pke(text_to_extract, method='tfidf' , dataset_path=dataset_path,\n",
    "            df_path=os.path.abspath(dataset_path + '../%s.df.tsv.gz' % dataset_name), top_k=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### YAKE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t1: send consistent sms (0.0001)\n",
      "\t2: data connect across (0.0002)\n",
      "\t3: use data build (0.0002)\n",
      "\t4: known data connect (0.0002)\n",
      "\t5: deliver experiences personalized (0.0002)\n",
      "\t6: enable intelligence marketing (0.0002)\n",
      "\t7: activate data capture (0.0002)\n",
      "\t8: create customer valuable (0.0002)\n",
      "\t9: engage create customer (0.0002)\n",
      "\t10: connect across sources (0.0002)\n",
      "\t11: transparent create customer (0.0002)\n",
      "\t12: valuable reach expand (0.0002)\n",
      "\t13: connect artificial social (0.0002)\n",
      "\t14: create business experiences (0.0002)\n",
      "\t15: measure optimize performance (0.0002)\n",
      "\t16: reach expand marketers (0.0002)\n",
      "\t17: expand marketers 2nd (0.0002)\n",
      "\t18: every lifecycle customer (0.0002)\n",
      "\t19: manage experiences customer (0.0003)\n",
      "\t20: experiences customer management (0.0003)\n"
     ]
    }
   ],
   "source": [
    "_ = kp_inference.extract_pke(text_to_extract, method='yake', top_k=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TextRank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Candidates are generated using 0.33-top\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t1: performance marketing total investment (0.0812)\n",
      "\t2: marketing data (0.0756)\n",
      "\t3: view unified devices multiple (0.0734)\n",
      "\t4: customer valuable (0.0709)\n",
      "\t5: experiences customer (0.0639)\n",
      "\t6: data build (0.0621)\n",
      "\t7: valuable interaction right (0.0620)\n",
      "\t8: data connect (0.0615)\n",
      "\t9: data (0.0454)\n",
      "\t10: customer (0.0421)\n"
     ]
    }
   ],
   "source": [
    "# define the set of valid Part-of-Speeches\n",
    "pos = {'NOUN', 'PROPN', 'ADJ'}\n",
    "\n",
    "# 1. create a TextRank extractor.\n",
    "extractor = pke.unsupervised.TextRank()\n",
    "\n",
    "# 2. load the content of the document.\n",
    "extractor.load_document(input=text_to_extract,\n",
    "                        language='en_core_web_sm',\n",
    "                        normalization=None)\n",
    "\n",
    "# 3. build the graph representation of the document and rank the words.\n",
    "#    Keyphrase candidates are composed from the 33-percent\n",
    "#    highest-ranked words.\n",
    "extractor.candidate_weighting(window=2,\n",
    "                              pos=pos,\n",
    "                              top_percent=0.33)\n",
    "\n",
    "# 4. get the 10-highest scored candidates as keyphrases\n",
    "keyphrases = extractor.get_n_best(n=10)\n",
    "for kp_id, kp in enumerate(keyphrases):\n",
    "    print('\\t%d: %s (%.4f)' % (kp_id+1, kp[0], kp[1]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
